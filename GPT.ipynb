{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzXVWUygyv56IFXJ4K3hRv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukhiyas/datasets/blob/main/GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QhCYSaKyfOWy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ],
      "metadata": {
        "id": "qeITZkrsgaV_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mukhiyas/datasets/main/fo.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkwE81LDg5qe",
        "outputId": "97de135d-1f07-4f6c-aa2e-a624bc17b5e8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-12 17:26:22--  https://raw.githubusercontent.com/mukhiyas/datasets/main/fo.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7919 (7.7K) [text/plain]\n",
            "Saving to: ‘fo.txt.1’\n",
            "\n",
            "\rfo.txt.1              0%[                    ]       0  --.-KB/s               \rfo.txt.1            100%[===================>]   7.73K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-12 17:26:22 (60.9 MB/s) - ‘fo.txt.1’ saved [7919/7919]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Text\n",
        "with open('fo.txt', 'r', encoding = \"UTF-8\" ) as f:\n",
        "    Text = f.read()\n",
        "\n",
        "    print(Text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F87WQ7Yag88L",
        "outputId": "2d7c5433-1231-46bb-d5df-8dc3a8336d02"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usually , he would be tearing around the living room , playing with his toys .\n",
            "but just one look at a minion sent him practically catatonic .\n",
            "that had been megan 's plan when she got him dressed earlier .\n",
            "he 'd seen the movie almost by mistake , considering he was a little young for the pg cartoon , but with older cousins , along with her brothers , mason was often exposed to things that were older .\n",
            "she liked to think being surrounded by adults and older kids was one reason why he was a such a good talker for his age .\n",
            "`` are n't you being a good boy ? ''\n",
            "she said .\n",
            "mason barely acknowledged her .\n",
            "instead , his baby blues remained focused on the television .\n",
            "since the movie was almost over , megan knew she better slip into the bedroom and finish getting ready .\n",
            "each time she looked into mason 's face , she was grateful that he looked nothing like his father .\n",
            "his platinum blond hair and blue eyes were completely hers .\n",
            "it was only his build that he was taking after his father .\n",
            "where megan was a diminutive 5'3 '' , davis was 6'1 '' and two hundred pounds .\n",
            "mason was already registering off the charts in height and weight according to his pediatrician .\n",
            "davis had seen mason only twice in his lifetime-the day he had been born and the day he came home from the hospital .\n",
            "after that , he had n't been interested in any of the pictures and emails megan sent .\n",
            "with his professional football career on the rise , davis had n't wanted to be shackled with the responsibilities of a baby .\n",
            "instead , he wanted to spend his time off the field partying until all hours of the night .\n",
            "he only paid child support when megan threatened to have his wages garnished .\n",
            "she dreaded the day when mason was old enough to ask about his father .\n",
            "she never wanted anything in the world to hurt him , and she knew that being rejected by his father would .\n",
            "with a sigh , she stepped into the dress and slid it over her hips .\n",
            "wrestling around to get the zipper all the way up caused her to huff and puff .\n",
            "standing back from the mirror , she turned to and fro to take in her appearance .\n",
            "she 'd always loved how the dress made her feel sexy , but at the same time was very respectable .\n",
            "while it boasted a sweetheart neckline , the hemline fell just below her knees .\n",
            "she put on her pearls-a high school graduation gift from her uncle aidan , or `` ankle '' , as she often called him .\n",
            "aidan was her mother 's baby brother and only son of the family .\n",
            "when she was born , he was only eight and a half .\n",
            "as the first grandchild , megan spent a lot of time with her grandparents , and that in turn , meant she spent a lot of time with aidan .\n",
            "he had devoted hours to holding her and spoiling her rotten .\n",
            "when it came time for her to talk , she just could n't seem to get `` uncle aidan '' out .\n",
            "instead , she called him `` ankle . ''\n",
            "it was a nickname that had stuck with him even now that he was thirty-four and married .\n",
            "while it had been no question that she wanted him as godfather for mason , she had been extremely honored when he and his wife , emma , had asked her to be their son , noah 's , godmother .\n",
            "she loved her newest cousin very much and planned to be the best godmother she could for him .\n",
            "as she stepped out of the bedroom , she found that mason had yet to move .\n",
            "`` okay buddy , time to go . ''\n",
            "when he started to whine , she shook her head .\n",
            "`` we have such a fun day ahead of us .\n",
            "it 's noah 's baptism , and then there 's a party at uncle aidan and aunt emma 's house . ''\n",
            "`` beau ? ''\n",
            "he asked .\n",
            "she laughed .\n",
            "`` yes , you 'll get to see and play with beau , too . ''\n",
            "as she went to the couch and picked him up , she could n't help finding it amusing that out of everyone he was going to see today , he was most excited about being with aidan and emma 's black lab , beau .\n",
            "one day when they had their own place again , she would get him a dog .\n",
            "he loved them too much to be denied .\n",
            "`` oomph , '' she muttered , as they started up the basement stairs .\n",
            "`` heawy ? ''\n",
            "he asked .\n",
            "`` yes , you 're getting to be such a big , heavy boy . ''\n",
            "when they made it to the kitchen , megan paused to catch her breath .\n",
            "she only had a second before her mother breezed in with sean , and her youngest brother , gavin .\n",
            "`` ready ? ''\n",
            "she asked .\n",
            "megan nodded .\n",
            "feeling like she was once again a teenager , she filed behind her parents as they headed into the garage .\n",
            "`` i want to drive , '' gavin said .\n",
            "with a smirk , sean replied , `` like i 'm gon na let you drive my car . ''\n",
            "he then slid into the driver 's seat as gavin reluctantly walked around to the passenger 's side .\n",
            "`` we 'll see you there in just a few , '' her mother called .\n",
            "sean acknowledged her with a two finger salute before cranking up and pulling down the driveway .\n",
            "megan worked to get mason into the car seat in her parents ' land rover .\n",
            "once he was safely strapped and buckled in , she hopped in beside him .\n",
            "her parents rattled along to each other as they made their way through the tree-lined suburbs where megan had grown up .\n",
            "while some might look on her as having a mark against her character being an unwed mother , she had lived a relatively non-rebellious life .\n",
            "even though she 'd been a cheerleader and ran with the popular crowd in school , she rarely partied to excess .\n",
            "instead , she had focused on getting good grades .\n",
            "at that time , she had her heart set on going to medical school and becoming a doctor .\n",
            "from the time she was a little girl , she had wanted nothing more than to help people .\n",
            "she was always mending birds with broken wings or trying to resuscitate squirrels who had been hit by cars .\n",
            "she ditched playing princess for playing `` hospital . ''\n",
            "her desire to become a doctor was why she needed the best scores and best activities and why she generally shunned any temptations to lead her off the right path .\n",
            "she had even managed to bypass the usual freshman craziness when she went off to the university of georgia .\n",
            "it was n't until she fell in love for the first time in her life that she threw everything away .\n",
            "sadly , she could n't say that her first love was davis , mason 's father .\n",
            "instead , it was another football player , this time a running back at uga , who captured and later broke her heart a year later .\n",
            "carsyn ran with the fast crowd , and when she was with him , she partied and drank too much .\n",
            "he was controlling and possessive , and he wanted all of her time .\n",
            "when she was with him , she had little time for studying .\n",
            "with her grades already in the toilet , she was unprepared for the emotional breakdown she experienced when carsyn broke up with her .\n",
            "devastated , she stopped going to class and ended up flunking the semester .\n",
            "by the time she got back on track with her grades , she had abandoned any hope of medical school .\n",
            "instead , she decided that she would become a nurse , which would fulfill her need to care for sick people .\n",
            "of course , her relationship with davis ended up derailing shortly before graduation when she got pregnant unexpectedly .\n",
            "she had to take several semesters off after mason was born .\n",
            "she was a few years off from when she had originally planned on graduating , but she was excited after everything had that had happened , she was finally finishing .\n",
            "her mother 's voice brought megan out of her thoughts .\n",
            "`` here we are , '' she said pleasantly .\n",
            "leaning forward in her seat , megan eyed the clock on the dashboard .\n",
            "she was n't surprised to see they had arrived half an hour before the baptism started .\n",
            "one thing her mother prided herself on was being on time and lending a hand .\n",
            "as they started into the church , her mother reached for mason .\n",
            "`` we 'll take him so you can go see if emma needs any help . ''\n",
            "megan bent over to kiss mason 's cheek .\n",
            "`` see you in a little while , sweetie . ''\n",
            "he grinned and then happily dodged her mother 's arms for her father 's instead , which made megan smile .\n",
            "he was such a man 's man already .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Char = (sorted(list(set(Text))))\n",
        "vocab_size = len(Char)\n",
        "\n",
        "print(Char)\n",
        "print(vocab_size)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw_4gEEeimB9",
        "outputId": "c67a9cb8-751a-4d1e-9094-4021d05745fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', \"'\", ',', '-', '.', '1', '3', '5', '6', '?', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch : i for i,ch in enumerate(Char)}\n",
        "itos = {i : ch for i,ch in enumerate(Char)}\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "print(encode(\"hii there\"))\n",
        "\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju5tcx_kjm84",
        "outputId": "1bcc0a88-9a4b-4ad7-9039-b44ecdfdab02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[19, 20, 20, 1, 31, 19, 16, 29, 16]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "\n",
        "data = torch.tensor(encode(Text), dtype=torch.long)\n",
        "\n",
        "print(data.shape, data.dtype)\n",
        "\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqutVcgRnjzj",
        "outputId": "e29dce61-f79f-45c9-8f86-56ba4af98dcd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([7919]) torch.int64\n",
            "tensor([32, 30, 32, 12, 23, 23, 36,  1,  3,  1, 19, 16,  1, 34, 26, 32, 23, 15,\n",
            "         1, 13, 16,  1, 31, 16, 12, 29, 20, 25, 18,  1, 12, 29, 26, 32, 25, 15,\n",
            "         1, 31, 19, 16,  1, 23, 20, 33, 20, 25, 18,  1, 29, 26, 26, 24,  1,  3,\n",
            "         1, 27, 23, 12, 36, 20, 25, 18,  1, 34, 20, 31, 19,  1, 19, 20, 30,  1,\n",
            "        31, 26, 36, 30,  1,  5,  0, 13, 32, 31,  1, 21, 32, 30, 31,  1, 26, 25,\n",
            "        16,  1, 23, 26, 26, 22,  1, 12, 31,  1, 12,  1, 24, 20, 25, 20, 26, 25,\n",
            "         1, 30, 16, 25, 31,  1, 19, 20, 24,  1, 27, 29, 12, 14, 31, 20, 14, 12,\n",
            "        23, 23, 36,  1, 14, 12, 31, 12, 31, 26, 25, 20, 14,  1,  5,  0, 31, 19,\n",
            "        12, 31,  1, 19, 12, 15,  1, 13, 16, 16, 25,  1, 24, 16, 18, 12, 25,  1,\n",
            "         2, 30,  1, 27, 23, 12, 25,  1, 34, 19, 16, 25,  1, 30, 19, 16,  1, 18,\n",
            "        26, 31,  1, 19, 20, 24,  1, 15, 29, 16, 30, 30, 16, 15,  1, 16, 12, 29,\n",
            "        23, 20, 16, 29,  1,  5,  0, 19, 16,  1,  2, 15,  1, 30, 16, 16, 25,  1,\n",
            "        31, 19, 16,  1, 24, 26, 33, 20, 16,  1, 12, 23, 24, 26, 30, 31,  1, 13,\n",
            "        36,  1, 24, 20, 30, 31, 12, 22, 16,  1,  3,  1, 14, 26, 25, 30, 20, 15,\n",
            "        16, 29, 20, 25, 18,  1, 19, 16,  1, 34, 12, 30,  1, 12,  1, 23, 20, 31,\n",
            "        31, 23, 16,  1, 36, 26, 32, 25, 18,  1, 17, 26, 29,  1, 31, 19, 16,  1,\n",
            "        27, 18,  1, 14, 12, 29, 31, 26, 26, 25,  1,  3,  1, 13, 32, 31,  1, 34,\n",
            "        20, 31, 19,  1, 26, 23, 15, 16, 29,  1, 14, 26, 32, 30, 20, 25, 30,  1,\n",
            "         3,  1, 12, 23, 26, 25, 18,  1, 34, 20, 31, 19,  1, 19, 16, 29,  1, 13,\n",
            "        29, 26, 31, 19, 16, 29, 30,  1,  3,  1, 24, 12, 30, 26, 25,  1, 34, 12,\n",
            "        30,  1, 26, 17, 31, 16, 25,  1, 16, 35, 27, 26, 30, 16, 15,  1, 31, 26,\n",
            "         1, 31, 19, 20, 25, 18, 30,  1, 31, 19, 12, 31,  1, 34, 16, 29, 16,  1,\n",
            "        26, 23, 15, 16, 29,  1,  5,  0, 30, 19, 16,  1, 23, 20, 22, 16, 15,  1,\n",
            "        31, 26,  1, 31, 19, 20, 25, 22,  1, 13, 16, 20, 25, 18,  1, 30, 32, 29,\n",
            "        29, 26, 32, 25, 15, 16, 15,  1, 13, 36,  1, 12, 15, 32, 23, 31, 30,  1,\n",
            "        12, 25, 15,  1, 26, 23, 15, 16, 29,  1, 22, 20, 15, 30,  1, 34, 12, 30,\n",
            "         1, 26, 25, 16,  1, 29, 16, 12, 30, 26, 25,  1, 34, 19, 36,  1, 19, 16,\n",
            "         1, 34, 12, 30,  1, 12,  1, 30, 32, 14, 19,  1, 12,  1, 18, 26, 26, 15,\n",
            "         1, 31, 12, 23, 22, 16, 29,  1, 17, 26, 29,  1, 19, 20, 30,  1, 12, 18,\n",
            "        16,  1,  5,  0, 11, 11,  1, 12, 29, 16,  1, 25,  2, 31,  1, 36, 26, 32,\n",
            "         1, 13, 16, 20, 25, 18,  1, 12,  1, 18, 26, 26, 15,  1, 13, 26, 36,  1,\n",
            "        10,  1,  2,  2,  0, 30, 19, 16,  1, 30, 12, 20, 15,  1,  5,  0, 24, 12,\n",
            "        30, 26, 25,  1, 13, 12, 29, 16, 23, 36,  1, 12, 14, 22, 25, 26, 34, 23,\n",
            "        16, 15, 18, 16, 15,  1, 19, 16, 29,  1,  5,  0, 20, 25, 30, 31, 16, 12,\n",
            "        15,  1,  3,  1, 19, 20, 30,  1, 13, 12, 13, 36,  1, 13, 23, 32, 16, 30,\n",
            "         1, 29, 16, 24, 12, 20, 25, 16, 15,  1, 17, 26, 14, 32, 30, 16, 15,  1,\n",
            "        26, 25,  1, 31, 19, 16,  1, 31, 16, 23, 16, 33, 20, 30, 20, 26, 25,  1,\n",
            "         5,  0, 30, 20, 25, 14, 16,  1, 31, 19, 16,  1, 24, 26, 33, 20, 16,  1,\n",
            "        34, 12, 30,  1, 12, 23, 24, 26, 30, 31,  1, 26, 33, 16, 29,  1,  3,  1,\n",
            "        24, 16, 18, 12, 25,  1, 22, 25, 16, 34,  1, 30, 19, 16,  1, 13, 16, 31,\n",
            "        31, 16, 29,  1, 30, 23, 20, 27,  1, 20, 25, 31, 26,  1, 31, 19, 16,  1,\n",
            "        13, 16, 15, 29, 26, 26, 24,  1, 12, 25, 15,  1, 17, 20, 25, 20, 30, 19,\n",
            "         1, 18, 16, 31, 31, 20, 25, 18,  1, 29, 16, 12, 15, 36,  1,  5,  0, 16,\n",
            "        12, 14, 19,  1, 31, 20, 24, 16,  1, 30, 19, 16,  1, 23, 26, 26, 22, 16,\n",
            "        15,  1, 20, 25, 31, 26,  1, 24, 12, 30, 26, 25,  1,  2, 30,  1, 17, 12,\n",
            "        14, 16,  1,  3,  1, 30, 19, 16,  1, 34, 12, 30,  1, 18, 29, 12, 31, 16,\n",
            "        17, 32, 23,  1, 31, 19, 12, 31,  1, 19, 16,  1, 23, 26, 26, 22, 16, 15,\n",
            "         1, 25, 26, 31, 19, 20, 25, 18,  1, 23, 20, 22, 16,  1, 19, 20, 30,  1,\n",
            "        17, 12, 31, 19, 16, 29,  1,  5,  0, 19, 20, 30,  1, 27, 23, 12, 31, 20,\n",
            "        25, 32, 24,  1, 13, 23, 26, 25, 15,  1, 19, 12, 20, 29,  1, 12, 25, 15,\n",
            "         1, 13, 23, 32, 16,  1, 16, 36, 16, 30,  1, 34, 16, 29, 16,  1, 14, 26,\n",
            "        24, 27, 23, 16, 31, 16, 23, 36,  1, 19, 16, 29, 30,  1,  5,  0, 20, 31,\n",
            "         1, 34, 12, 30,  1, 26, 25, 23, 36,  1, 19, 20, 30,  1, 13, 32, 20, 23,\n",
            "        15,  1, 31, 19, 12, 31,  1, 19, 16,  1, 34, 12, 30,  1, 31, 12, 22, 20,\n",
            "        25, 18,  1, 12, 17, 31, 16, 29,  1, 19, 20, 30,  1, 17, 12, 31, 19, 16,\n",
            "        29,  1,  5,  0, 34, 19, 16, 29, 16,  1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "\n",
        "\n",
        "train_data = data[:n]\n",
        "val_daya = data[n:]\n",
        "\n",
        "print(train_data)\n",
        "print(val_daya)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLwjnqQtqhGq",
        "outputId": "b646b6c7-e0ec-4b17-fba2-775e12cb4f47"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([32, 30, 32,  ..., 26, 25,  1])\n",
            "tensor([18, 29, 12, 15, 32, 12, 31, 20, 25, 18,  1,  3,  1, 13, 32, 31,  1, 30,\n",
            "        19, 16,  1, 34, 12, 30,  1, 16, 35, 14, 20, 31, 16, 15,  1, 12, 17, 31,\n",
            "        16, 29,  1, 16, 33, 16, 29, 36, 31, 19, 20, 25, 18,  1, 19, 12, 15,  1,\n",
            "        31, 19, 12, 31,  1, 19, 12, 15,  1, 19, 12, 27, 27, 16, 25, 16, 15,  1,\n",
            "         3,  1, 30, 19, 16,  1, 34, 12, 30,  1, 17, 20, 25, 12, 23, 23, 36,  1,\n",
            "        17, 20, 25, 20, 30, 19, 20, 25, 18,  1,  5,  0, 19, 16, 29,  1, 24, 26,\n",
            "        31, 19, 16, 29,  1,  2, 30,  1, 33, 26, 20, 14, 16,  1, 13, 29, 26, 32,\n",
            "        18, 19, 31,  1, 24, 16, 18, 12, 25,  1, 26, 32, 31,  1, 26, 17,  1, 19,\n",
            "        16, 29,  1, 31, 19, 26, 32, 18, 19, 31, 30,  1,  5,  0, 11, 11,  1, 19,\n",
            "        16, 29, 16,  1, 34, 16,  1, 12, 29, 16,  1,  3,  1,  2,  2,  1, 30, 19,\n",
            "        16,  1, 30, 12, 20, 15,  1, 27, 23, 16, 12, 30, 12, 25, 31, 23, 36,  1,\n",
            "         5,  0, 23, 16, 12, 25, 20, 25, 18,  1, 17, 26, 29, 34, 12, 29, 15,  1,\n",
            "        20, 25,  1, 19, 16, 29,  1, 30, 16, 12, 31,  1,  3,  1, 24, 16, 18, 12,\n",
            "        25,  1, 16, 36, 16, 15,  1, 31, 19, 16,  1, 14, 23, 26, 14, 22,  1, 26,\n",
            "        25,  1, 31, 19, 16,  1, 15, 12, 30, 19, 13, 26, 12, 29, 15,  1,  5,  0,\n",
            "        30, 19, 16,  1, 34, 12, 30,  1, 25,  2, 31,  1, 30, 32, 29, 27, 29, 20,\n",
            "        30, 16, 15,  1, 31, 26,  1, 30, 16, 16,  1, 31, 19, 16, 36,  1, 19, 12,\n",
            "        15,  1, 12, 29, 29, 20, 33, 16, 15,  1, 19, 12, 23, 17,  1, 12, 25,  1,\n",
            "        19, 26, 32, 29,  1, 13, 16, 17, 26, 29, 16,  1, 31, 19, 16,  1, 13, 12,\n",
            "        27, 31, 20, 30, 24,  1, 30, 31, 12, 29, 31, 16, 15,  1,  5,  0, 26, 25,\n",
            "        16,  1, 31, 19, 20, 25, 18,  1, 19, 16, 29,  1, 24, 26, 31, 19, 16, 29,\n",
            "         1, 27, 29, 20, 15, 16, 15,  1, 19, 16, 29, 30, 16, 23, 17,  1, 26, 25,\n",
            "         1, 34, 12, 30,  1, 13, 16, 20, 25, 18,  1, 26, 25,  1, 31, 20, 24, 16,\n",
            "         1, 12, 25, 15,  1, 23, 16, 25, 15, 20, 25, 18,  1, 12,  1, 19, 12, 25,\n",
            "        15,  1,  5,  0, 12, 30,  1, 31, 19, 16, 36,  1, 30, 31, 12, 29, 31, 16,\n",
            "        15,  1, 20, 25, 31, 26,  1, 31, 19, 16,  1, 14, 19, 32, 29, 14, 19,  1,\n",
            "         3,  1, 19, 16, 29,  1, 24, 26, 31, 19, 16, 29,  1, 29, 16, 12, 14, 19,\n",
            "        16, 15,  1, 17, 26, 29,  1, 24, 12, 30, 26, 25,  1,  5,  0, 11, 11,  1,\n",
            "        34, 16,  1,  2, 23, 23,  1, 31, 12, 22, 16,  1, 19, 20, 24,  1, 30, 26,\n",
            "         1, 36, 26, 32,  1, 14, 12, 25,  1, 18, 26,  1, 30, 16, 16,  1, 20, 17,\n",
            "         1, 16, 24, 24, 12,  1, 25, 16, 16, 15, 30,  1, 12, 25, 36,  1, 19, 16,\n",
            "        23, 27,  1,  5,  1,  2,  2,  0, 24, 16, 18, 12, 25,  1, 13, 16, 25, 31,\n",
            "         1, 26, 33, 16, 29,  1, 31, 26,  1, 22, 20, 30, 30,  1, 24, 12, 30, 26,\n",
            "        25,  1,  2, 30,  1, 14, 19, 16, 16, 22,  1,  5,  0, 11, 11,  1, 30, 16,\n",
            "        16,  1, 36, 26, 32,  1, 20, 25,  1, 12,  1, 23, 20, 31, 31, 23, 16,  1,\n",
            "        34, 19, 20, 23, 16,  1,  3,  1, 30, 34, 16, 16, 31, 20, 16,  1,  5,  1,\n",
            "         2,  2,  0, 19, 16,  1, 18, 29, 20, 25, 25, 16, 15,  1, 12, 25, 15,  1,\n",
            "        31, 19, 16, 25,  1, 19, 12, 27, 27, 20, 23, 36,  1, 15, 26, 15, 18, 16,\n",
            "        15,  1, 19, 16, 29,  1, 24, 26, 31, 19, 16, 29,  1,  2, 30,  1, 12, 29,\n",
            "        24, 30,  1, 17, 26, 29,  1, 19, 16, 29,  1, 17, 12, 31, 19, 16, 29,  1,\n",
            "         2, 30,  1, 20, 25, 30, 31, 16, 12, 15,  1,  3,  1, 34, 19, 20, 14, 19,\n",
            "         1, 24, 12, 15, 16,  1, 24, 16, 18, 12, 25,  1, 30, 24, 20, 23, 16,  1,\n",
            "         5,  0, 19, 16,  1, 34, 12, 30,  1, 30, 32, 14, 19,  1, 12,  1, 24, 12,\n",
            "        25,  1,  2, 30,  1, 24, 12, 25,  1, 12, 23, 29, 16, 12, 15, 36,  1,  5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "block_size = 8\n",
        "train_data[:block_size+1]\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQwULTrsqUGi",
        "outputId": "a4bd7d98-1e35-45da-9b73-a4f04524cf63"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([32, 30, 32, 12, 23, 23, 36,  1])\n",
            "tensor([30, 32, 12, 23, 23, 36,  1,  3])\n",
            "when input is tensor([32, 30, 32, 12, 23, 23, 36,  1]) the target: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "# generate a small batch of data of inputs x and targets y\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "  for t in range(block_size): # time dimension\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PB6VgBZZukq3",
        "outputId": "90196312-759b-4609-c787-4b5fcd49c70a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[12, 18, 16, 30,  1, 18, 12, 29],\n",
            "        [30, 32, 14, 19,  1, 12,  1, 17],\n",
            "        [18, 19, 31,  1, 23, 26, 26, 22],\n",
            "        [31,  1, 12,  1, 23, 26, 31,  1]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[18, 16, 30,  1, 18, 12, 29, 25],\n",
            "        [32, 14, 19,  1, 12,  1, 17, 32],\n",
            "        [19, 31,  1, 23, 26, 26, 22,  1],\n",
            "        [ 1, 12,  1, 23, 26, 31,  1, 26]])\n",
            "----\n",
            "when input is [31, 1, 12, 1, 23, 26, 31, 1] the target: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "rl1GlvXv55T4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine we have a machine learning model that we want to test. We want to see how well it's doing on both the training data and the validation data.\n",
        "\n",
        "First, we have a special tool called @torch.no_grad(). This tool helps us test the model without changing anything inside it. It's like putting the model in a special \"viewing mode\" where it can't make any changes.\n",
        "\n",
        "Now, we have a function called estimate_loss(). This function is going to help us see how well the model is doing.\n",
        "\n",
        "The function starts by creating an empty dictionary called out. This is where we'll store the results.\n",
        "Next, the function puts the model in \"evaluation mode\" using model.eval(). This is important because it changes how the model behaves a bit, to help us get better results.\n",
        "Then, the function looks at two different sets of data: the \"training\" data and the \"validation\" data.\n",
        "For each set of data, the function creates a list called losses to store the individual losses.\n",
        "It then goes through the data in small batches, using a function called get_batch() to get the input and target for each batch.\n",
        "For each batch, the function runs the model and calculates the loss. It stores this loss in the losses list.\n",
        "Let's visualize this part:\n",
        "\n",
        "\n",
        "Copy code\n",
        "+-------------------+\n",
        "|     Training     |\n",
        "|  +----------+    |\n",
        "|  | Batch 1  |    |\n",
        "|  +----------+    |\n",
        "|  | Batch 2  |    |\n",
        "|  +----------+    |\n",
        "|       ...        |\n",
        "+-------------------+\n",
        "\n",
        "+-------------------+\n",
        "|     Validation    |\n",
        "|  +----------+    |\n",
        "|  | Batch 1  |    |\n",
        "|  +----------+    |\n",
        "|  | Batch 2  |    |\n",
        "|  +----------+    |\n",
        "|       ...        |\n",
        "+-------------------+\n",
        "After going through all the batches, the function calculates the average loss for each set of data (training and validation) and stores the results in the out dictionary.\n",
        "Finally, the function puts the model back in \"training mode\" using model.train(), since we're done testing.\n",
        "The function returns the out dictionary, which contains the average losses for the training and validation sets.\n",
        "The key thing to understand here is that the function is helping us evaluate how well the model is performing, without actually changing the model itself. This is a common step in the machine learning process, and the @torch.no_grad() tool is very useful for making sure we don't accidentally update the model during this evaluation.\n",
        "\n"
      ],
      "metadata": {
        "id": "SxJiaMhx8gGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The \"Head\" part uses the special tools (key, query, value, tril, dropout) to process the information (x) and create a new, improved version of the information (out).\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"\n",
        "    One head of self-attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # These are like special tools that the model can use. It uses the key, query, and value tools to examine the information and figure out which parts are more important.\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        # This is a special triangle-shaped thing that helps the model. It uses the tril tool to help it focus on the parts that are more important.\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        # This is a tool that can randomly hide some information, to help the model.It uses the dropout tool to randomly hide some of the information, to help it learn better.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        #Finally, the machine uses all this information to create a new, improved version of the original information, which it sends out as the \"output\".\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # The model uses the special tools to get the key, query, and value\n",
        "        k = self.key(x)    # (B, T, C)\n",
        "        q = self.query(x)  # (B, T, C)\n",
        "\n",
        "        # The model uses the key and query to figure out how important each part is\n",
        "        wei = q @ k.transpose(-2, -1) * (1 / C**0.5)  # (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # The model uses the weighted value to get the final output\n",
        "        v = self.value(x)  # (B, T, C)\n",
        "        out = wei @ v      # (B, T, C)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "9p-b7iJk92bW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you have a machine that can understand and process information. This machine has a special \"Head\" part that helps it focus on the important parts of the information.\n",
        "\n",
        "The \"Head\" part has some special tools:\n",
        "key, query, and value: These are like special lenses that the machine uses to examine the information.\n",
        "tril: This is a special triangle-shaped tool that helps the machine figure out which parts of the information are more important.\n",
        "dropout: This is a tool that can randomly hide some of the information, to help the machine learn better.\n",
        "When the machine gets some information (let's call it x), it uses these special tools to do some magic:\n",
        "It uses the key, query, and value tools to examine the information and figure out which parts are more important.\n",
        "It uses the tril tool to help it focus on the parts that are more important.\n",
        "It uses the dropout tool to randomly hide some of the information, to help it learn better.\n",
        "Finally, the machine uses all this information to create a new, improved version of the original information, which it sends out as the \"output\".\n",
        "Visualization:\n",
        "\n",
        "Imagine the information (x) is a big box, and the \"Head\" part of the machine is like a smaller box inside it. Here's what happens:\n",
        "\n",
        "\n",
        "Copy code\n",
        "+-------------------+\n",
        "|        x         |\n",
        "|  +----------+    |\n",
        "|  |   Head   |    |\n",
        "|  | +-----+ |    |\n",
        "|  | | key | |    |\n",
        "|  | +-----+ |    |\n",
        "|  | +-----+ |    |\n",
        "|  | |query| |    |\n",
        "|  | +-----+ |    |\n",
        "|  | +-----+ |    |\n",
        "|  | |value| |    |\n",
        "|  | +-----+ |    |\n",
        "|  | +-----+ |    |\n",
        "|  | |tril | |    |\n",
        "|  | +-----+ |    |\n",
        "|  | +-----+ |    |\n",
        "|  | |dropout| |    |\n",
        "|  | +-----+ |    |\n",
        "|  +----------+    |\n",
        "+-------------------+\n",
        "\n",
        "    +---------+\n",
        "    |   out   |\n",
        "    +---------+\n",
        "The \"Head\" part uses the special tools (key, query, value, tril, dropout) to process the information (x) and create a new, improved version of the information (out).\n",
        "\n",
        "\n",
        "Sure, let's break down this part of the code and explain it in simple terms:\n",
        "\n",
        "python\n",
        "\n",
        "\n",
        "Copy code\n",
        "def forward(self, x):\n",
        "    B, T, C = x.shape\n",
        "    \n",
        "    # The model uses the special tools to get the key, query, and value\n",
        "    k = self.key(x)    # (B, T, C)\n",
        "    q = self.query(x)  # (B, T, C)\n",
        "The first line B, T, C = x.shape is getting the size of the input x. It's like the machine is measuring the size of the information it's been given.\n",
        "Next, the machine uses the key and query tools to examine the information. It's like the machine is looking at the information through two different lenses.\n",
        "python\n",
        "\n",
        "\n",
        "Copy code\n",
        "    # The model uses the key and query to figure out how important each part is\n",
        "    wei = q @ k.transpose(-2, -1) * (1 / C**0.5)  # (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "Now, the machine uses the key and query information to figure out how important each part of the information is. It does this by:\n",
        "Multiplying the query and key information (in a special way)\n",
        "Using the tril tool to focus on the important parts\n",
        "Applying a \"softmax\" function to make sure the importance values add up to 1\n",
        "Using the dropout tool to randomly hide some of the information\n",
        "This step helps the machine understand which parts of the information are more important than others.\n",
        "\n",
        "python\n",
        "\n",
        "\n",
        "Copy code\n",
        "    # The model uses the weighted value to get the final output\n",
        "    v = self.value(x)  # (B, T, C)\n",
        "    out = wei @ v      # (B, T, C)\n",
        "\n",
        "    return out\n",
        "Finally, the machine uses the value tool to get the actual information it needs, and then combines this with the \"importance\" information it just calculated. This gives the machine the final output it needs.\n",
        "In summary, the forward function is the core of the \"Head\" part of the machine. It takes in some information, uses the special tools to process and understand that information, and then produces a new, improved version of the information as the output."
      ],
      "metadata": {
        "id": "dzFV3nlN-ggR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multiple heads of self-attention in parallel.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Create a bunch of \"Head\" parts and put them in a list\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "        # This is a special tool to combine the outputs from all the \"Head\" parts\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        # This is a tool that can randomly hide some information, to help the model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Run the input through all the \"Head\" parts in parallel\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        # Use the special tool to combine the outputs from all the \"Head\" parts\n",
        "        out = self.dropout(self.proj(out))\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "l93kcX4mBtW2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation in simple terms:\n",
        "\n",
        "The MultiHeadAttention class is like a bigger machine that has multiple \"Head\" parts inside it.\n",
        "In the __init__ method:\n",
        "The machine creates a bunch of \"Head\" parts and puts them in a list. The number of \"Head\" parts is determined by the num_heads parameter.\n",
        "The machine also has a special tool called proj that can combine the outputs from all the \"Head\" parts.\n",
        "The machine has a dropout tool that can randomly hide some information, to help it learn better.\n",
        "In the forward method:\n",
        "The machine takes some input x and runs it through all the \"Head\" parts in parallel.\n",
        "The machine then uses the proj tool to combine the outputs from all the \"Head\" parts into a single output.\n",
        "The machine also uses the dropout tool to randomly hide some of the information in the output, to help it learn better.\n",
        "The final output is returned by the forward method.\n",
        "Visualizing the MultiHeadAttention class:\n",
        "\n",
        "\n",
        "Copy code\n",
        "+-------------------+\n",
        "|        x         |\n",
        "|  +----------+    |\n",
        "|  | Head 1   |    |\n",
        "|  +----------+    |\n",
        "|  | Head 2   |    |\n",
        "|  +----------+    |\n",
        "|  |  ...    |    |\n",
        "|  | Head N   |    |\n",
        "|  +----------+    |\n",
        "|  +----------+    |\n",
        "|  |  proj   |    |\n",
        "|  +----------+    |\n",
        "|  +----------+    |\n",
        "|  | dropout |    |\n",
        "|  +----------+    |\n",
        "+-------------------+\n",
        "\n",
        "    +---------+\n",
        "    |   out   |\n",
        "    +---------+\n",
        "The MultiHeadAttention class takes the input x and runs it through multiple \"Head\" parts in parallel. The outputs from these \"Head\" parts are then combined using the proj tool, and the dropout tool is applied to the final output.\n",
        "\n",
        "By using multiple \"Head\" parts, the MultiHeadAttention class can learn to focus on different aspects of the input, which can help improve the overall performance of the machine learning model."
      ],
      "metadata": {
        "id": "yKUR-1N2CNga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down the concepts of single-head and multi-head attention with a simple visual example.\n",
        "\n",
        "Single-Head Attention:\n",
        "In single-head attention, the input is transformed into three sets of vectors: queries, keys, and values. These vectors are then used to compute attention scores, which indicate how much focus each word should give to the other words in the sequence. Finally, the attention scores are used to compute a weighted sum of the values, producing the output.\n",
        "\n",
        "Here's a simplified visual representation of the process:\n",
        "\n",
        "Input Transformation:\n",
        "The input sequence is transformed into three sets of vectors: queries, keys, and values.\n",
        "Each word in the input sequence is represented by a vector.\n",
        "less\n",
        "Copy code\n",
        "Input Sequence (5 words):\n",
        "[Word1] [Word2] [Word3] [Word4] [Word5]\n",
        "\n",
        "Queries:\n",
        "[Q1]    [Q2]    [Q3]    [Q4]    [Q5]\n",
        "\n",
        "Keys:\n",
        "[K1]    [K2]    [K3]    [K4]    [K5]\n",
        "\n",
        "Values:\n",
        "[V1]    [V2]    [V3]    [V4]    [V5]\n",
        "Attention Scores:\n",
        "Each query vector computes a dot product with every key vector, resulting in a score that indicates the importance of each word in the sequence.\n",
        "less\n",
        "Copy code\n",
        "Attention Scores:\n",
        "[Q1·K1] [Q1·K2] [Q1·K3] [Q1·K4] [Q1·K5]\n",
        "[Q2·K1] [Q2·K2] [Q2·K3] [Q2·K4] [Q2·K5]\n",
        "...\n",
        "[Q5·K1] [Q5·K2] [Q5·K3] [Q5·K4] [Q5·K5]\n",
        "Attention Weights:\n",
        "\n",
        "The attention scores are scaled and normalized using softmax to obtain attention weights.\n",
        "Weighted Sum:\n",
        "\n",
        "The attention weights are used to compute a weighted sum of the values, producing the output.\n",
        "Multi-Head Attention:\n",
        "In multi-head attention, the input is processed by multiple independent attention mechanisms, or \"heads\". Each head performs the same attention computation, but with different learned parameters. The outputs of all heads are then concatenated and linearly transformed to produce the final output.\n",
        "\n",
        "Here's a simplified visual representation of the process:\n",
        "\n",
        "Multiple Heads:\n",
        "\n",
        "The input sequence is processed by multiple attention heads in parallel.\n",
        "Each attention head performs its own query, key, value transformation and computes attention scores independently.\n",
        "Concatenation:\n",
        "\n",
        "The outputs of all attention heads are concatenated along the feature dimension.\n",
        "Linear Transformation:\n",
        "\n",
        "The concatenated outputs are passed through a linear layer to produce the final output.\n",
        "By using multiple heads, the model can capture different aspects of the input sequence simultaneously, enhancing its ability to learn complex patterns and relationships.\n",
        "\n",
        "I hope this visual explanation helps clarify the concepts of single-head and multi-head attention! Let me know if you have further questions!"
      ],
      "metadata": {
        "id": "PxqcBTBmZEPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's continue the explanation using the provided code for the MultiHeadAttention class.\n",
        "\n",
        "Multi-Head Attention:\n",
        "Initialization:\n",
        "\n",
        "In the __init__ method, we initialize the multi-head attention module.\n",
        "We create num_heads instances of the Head class and store them in a list called self.heads.\n",
        "Each Head instance represents an independent attention mechanism or head.\n",
        "We also initialize a linear transformation layer called self.proj to combine the outputs of all the heads.\n",
        "Additionally, we initialize a dropout layer called self.dropout to randomly mask some information during training.\n",
        "Forward Pass:\n",
        "\n",
        "In the forward method, we apply the multi-head attention mechanism to the input tensor x.\n",
        "We pass the input tensor x through each head in parallel using list comprehension: [h(x) for h in self.heads].\n",
        "Each head processes the input independently and produces an output tensor.\n",
        "We concatenate the output tensors of all heads along the feature dimension using torch.cat, resulting in a tensor with shape (batch_size, sequence_length, n_embd * num_heads).\n",
        "Next, we apply dropout to the concatenated tensor to randomly mask some information, helping prevent overfitting during training.\n",
        "Finally, we pass the masked tensor through the linear transformation layer self.proj to combine the outputs from all heads into a single tensor.\n",
        "The resulting tensor represents the final output of the multi-head attention mechanism.\n",
        "Illustration:\n",
        "Let's visualize the multi-head attention process using a simple diagram:\n",
        "\n",
        "mathematica\n",
        "Copy code\n",
        "  Input\n",
        "   |\n",
        "   v\n",
        "  ---------------------\n",
        " |      Multi-Head     |\n",
        " |      Attention      |\n",
        " |---------------------|\n",
        " |                     |\n",
        " |       [ Head 1 ]    |\n",
        " |       [ Head 2 ]    |\n",
        " |       [  ...  ]     |\n",
        " |       [ Head N ]    |\n",
        " |                     |\n",
        "  ---------------------\n",
        "          |\n",
        "          v\n",
        "  Concatenated Outputs\n",
        "          |\n",
        "          v\n",
        "      Linear Projection\n",
        "          |\n",
        "          v\n",
        "      Output Tensor\n",
        "In this illustration:\n",
        "\n",
        "The input tensor flows into the multi-head attention module.\n",
        "Each head processes the input independently and produces an output.\n",
        "The outputs of all heads are concatenated along the feature dimension.\n",
        "The concatenated outputs are passed through a linear transformation layer to produce the final output tensor.\n",
        "This diagram represents the flow of data through the MultiHeadAttention module, illustrating how multiple attention heads work in parallel to capture different aspects of the input."
      ],
      "metadata": {
        "id": "jHLwtPBlaHGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" A simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, dropout=0.1):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd, dropout=dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "rdyJ1V--yTwi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNLc5UP0y3t0",
        "outputId": "b341e869-f5b5-4ce3-8673-e2a4acb29fe9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.20471 M parameters\n",
            "step 0: train loss 3.8568, val loss 3.8403\n",
            "step 100: train loss 2.4864, val loss 2.4356\n",
            "step 200: train loss 2.3455, val loss 2.2992\n",
            "step 300: train loss 2.2913, val loss 2.2825\n",
            "step 400: train loss 2.2156, val loss 2.2417\n",
            "step 500: train loss 2.1824, val loss 2.1605\n",
            "step 600: train loss 2.1821, val loss 2.1549\n",
            "step 700: train loss 2.1252, val loss 2.1921\n",
            "step 800: train loss 2.1181, val loss 2.1104\n",
            "step 900: train loss 2.0687, val loss 2.1105\n",
            "step 1000: train loss 2.0100, val loss 2.1073\n",
            "step 1100: train loss 1.9777, val loss 2.0145\n",
            "step 1200: train loss 1.9657, val loss 2.0344\n",
            "step 1300: train loss 1.9245, val loss 2.0008\n",
            "step 1400: train loss 1.9503, val loss 2.0073\n",
            "step 1500: train loss 1.9048, val loss 2.0047\n",
            "step 1600: train loss 1.9076, val loss 1.9834\n",
            "step 1700: train loss 1.9060, val loss 2.0109\n",
            "step 1800: train loss 1.8484, val loss 1.9839\n",
            "step 1900: train loss 1.8283, val loss 1.9893\n",
            "step 2000: train loss 1.8441, val loss 1.9121\n",
            "step 2100: train loss 1.7855, val loss 1.9192\n",
            "step 2200: train loss 1.8371, val loss 1.9449\n",
            "step 2300: train loss 1.7772, val loss 1.8790\n",
            "step 2400: train loss 1.7566, val loss 1.9088\n",
            "step 2500: train loss 1.7744, val loss 1.9372\n",
            "step 2600: train loss 1.7983, val loss 1.9081\n",
            "step 2700: train loss 1.7092, val loss 1.9180\n",
            "step 2800: train loss 1.7232, val loss 1.9409\n",
            "step 2900: train loss 1.7372, val loss 1.9138\n",
            "step 3000: train loss 1.7224, val loss 1.8793\n",
            "step 3100: train loss 1.6772, val loss 1.8754\n",
            "step 3200: train loss 1.6584, val loss 1.9051\n",
            "step 3300: train loss 1.6715, val loss 1.8505\n",
            "step 3400: train loss 1.6431, val loss 1.9229\n",
            "step 3500: train loss 1.6672, val loss 1.8591\n",
            "step 3600: train loss 1.6369, val loss 1.9176\n",
            "step 3700: train loss 1.5983, val loss 1.8734\n",
            "step 3800: train loss 1.6573, val loss 1.8948\n",
            "step 3900: train loss 1.5976, val loss 1.8799\n",
            "step 4000: train loss 1.5827, val loss 1.8905\n",
            "step 4100: train loss 1.5656, val loss 1.8843\n",
            "step 4200: train loss 1.5828, val loss 1.8673\n",
            "step 4300: train loss 1.5681, val loss 1.9029\n",
            "step 4400: train loss 1.5460, val loss 1.8924\n",
            "step 4500: train loss 1.5474, val loss 1.8408\n",
            "step 4600: train loss 1.5044, val loss 1.9616\n",
            "step 4700: train loss 1.5146, val loss 1.9684\n",
            "step 4800: train loss 1.5063, val loss 1.9360\n",
            "step 4900: train loss 1.5006, val loss 1.8655\n",
            "step 4999: train loss 1.4999, val loss 1.9109\n",
            "\n",
            "as the become him day , mut to sphirt her 's , he was-al spactndrel-for se fortion than \n",
            "with he droodged'-\n",
            "as see graduting for nothing kn-ro.\n",
            "and the couwdmor , she she 'd wes along plantling spent course .\n",
            "she .\n",
            "she 'd , him `` .\n",
            "when one gotting two huddy bues agaife just hen get tanted .\n",
            "`` ouse .\n",
            "she was who malk .\n",
            "`` okan the to time ,  , `` out bedrt 's she just drebantever ,nven as long and broe the bely tw .\n",
            "`` .\n",
            "thart bacter .\n",
            "it sas with blaunged tolgedger ,dame son sto the demesternedroung ''  ''\n",
            "when as pnorted for cousese , dou 'd , garan n'tedmegan thiry .\n",
            "with he toughaly .\n",
            "she was was a .\n",
            "the decidne he clal she had .\n",
            "megan , the was they born he 'd thy that been meare . ''\n",
            "`` out forgest finun long bore grades . ''\n",
            "he was heaving heot been that of un5ted born and the asked .\n",
            "his her .\n",
            "henather , she was they he just wo lufter gradunested a supperaged .\n",
            "she 'd ekome mother , can the the , she , she 'd bready inster hab stusing to hujgedred .\n",
            "byest she , she 'd , at seeady him `` , yot wher as a seidaned in him that .\n",
            "when her hom get ,  ''\n",
            "he was that onnat her wa-ted for gan nesseed in thed move whonly her baptereded would .\n",
            "his madand him .\n",
            "instead , she we migan , he he dre-doniame , and hough .\n",
            "and him .\n",
            "wwas when.\n",
            "`` une , she get 'd sche day , megan 's , she drejedred init and the temmean ceaarsed o' unprepparted , been her was when hat chom thra ,- 'laved .\n",
            "insibe .\n",
            "`` ole knould the of the dound with her poked the stated be .\n",
            "`` eenyedgorn , day that ca?vinges the dove 'rounged he kow undread .\n",
            "she was theirt in the yectun't ic't oke wages very , she had the `` uneqped , she was time with him ding 3ok .\n",
            "her pur th .\n",
            "wehing tioth her knew that po nasespent , dother .\n",
            "when newstal cresped hom a see todugh , paunned bir .\n",
            "`` with him ove .\n",
            "she 'd got threaged up inly .\n",
            "she 'd beed mother best couth topatly in tiye having ran ''d bue drive .\n",
            "with hewring and n't capntedly .\n",
            "witiked .\n",
            "off up .\n",
            "it an't `` toy ? ''\n",
            "he dreadup\n",
            "insteady it `` uncked\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how an optimizer minimizes the loss:\n",
        "\n",
        "Initialization: We start by initializing the parameters (weights and biases) of the linear regression model randomly or with some predefined values.\n",
        "\n",
        "Forward Pass: We pass the input data through the model to obtain predictions.\n",
        "\n",
        "Loss Computation: We compute the loss by comparing the predicted values with the actual values using the MSE loss function.\n",
        "\n",
        "Backpropagation: We compute the gradients of the loss function with respect to the parameters of the model using backpropagation. These gradients indicate the direction and magnitude of the parameter updates needed to reduce the loss.\n",
        "\n",
        "Parameter Update: The optimizer uses these gradients to update the parameters of the model. It adjusts the parameters in the direction that minimizes the loss, typically by taking a step proportional to the negative gradient.\n",
        "\n",
        "Iteration: Steps 2-5 are repeated for multiple iterations or epochs, gradually reducing the loss and improving the model's performance.\n",
        "\n",
        "Let's illustrate this process with a simple example:\n",
        "\n",
        "Suppose we have a dataset with one feature (input) and one target variable (output), and we want to fit a linear regression model to this data.\n",
        "\n",
        "Input data (x): [1, 2, 3, 4, 5]\n",
        "Target variable (y): [2, 4, 6, 8, 10]\n",
        "\n",
        "Initialization: We randomly initialize the parameters of the linear regression model: slope (weight) = 1 and intercept (bias) = 0.\n",
        "\n",
        "Forward Pass: We pass the input data through the model to obtain predictions.\n",
        "\n",
        "Loss Computation: We compute the mean squared error (MSE) loss between the predicted values and the actual values.\n",
        "\n",
        "Backpropagation: We compute the gradients of the loss function with respect to the parameters (slope and intercept) using backpropagation.\n",
        "\n",
        "Parameter Update: The optimizer uses these gradients to update the parameters of the model (slope and intercept) in the direction that minimizes the loss.\n",
        "\n",
        "Iteration: Steps 2-5 are repeated for multiple iterations or epochs, gradually reducing the loss and improving the model's fit to the data.\n",
        "\n",
        "By iteratively adjusting the parameters based on the computed gradients, the optimizer effectively minimizes the loss and improves the model's performance on the given dataset."
      ],
      "metadata": {
        "id": "Pro4LI2L1HfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8oB_oAo41DCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "p.numel() returns the number of elements in the parameter p.\n",
        "\n",
        "In PyTorch, p is typically a parameter tensor within a neural network model. The numel() function calculates the total number of elements in the tensor, regardless of its shape or dimensions.\n",
        "\n",
        "For example, if p is a tensor of shape (3, 4), p.numel() would return 12, as there are a total of 12 elements in the tensor.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Let's break down the line print(decode(m.generate(context, max_new_tokens=2000)[0].tolist())):\n",
        "\n",
        "m.generate(context, max_new_tokens=2000):\n",
        "\n",
        "This part of the code generates new text based on the provided context using the generate method of the BigramLanguageModel instance m.\n",
        "context: This is the initial context for text generation, which we defined earlier as a tensor filled with zeros.\n",
        "max_new_tokens=2000: This specifies the maximum number of new tokens (words or characters) to generate.\n",
        "[0]:\n",
        "\n",
        "The generated text is returned as a tensor. Since we're only generating one sequence of text, we access the first element of the returned tensor using [0].\n",
        ".tolist():\n",
        "\n",
        "The .tolist() method converts the tensor to a Python list. This is done because the tensor contains integer indices representing tokens, and converting it to a list makes it easier to work with for decoding.\n",
        "decode():\n",
        "\n",
        "The decode() function is presumably a custom function defined elsewhere in the codebase. It takes a list of token indices and converts them into human-readable text.\n",
        "It likely involves looking up each token index in a vocabulary to retrieve the corresponding word or character.\n",
        "print():\n",
        "\n",
        "Finally, the print() function is used to display the decoded text to the console."
      ],
      "metadata": {
        "id": "WtElDnQc2uSR"
      }
    }
  ]
}